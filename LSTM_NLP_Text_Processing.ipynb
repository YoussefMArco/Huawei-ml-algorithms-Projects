{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jij1zNGgXf8"
      },
      "source": [
        "**Keras for Beginners: Implementing a Recurrent Neural Network**\n",
        "\n",
        "https://victorzhou.com/blog/keras-rnn-tutorial/\n",
        "\n",
        "**The Problem: Classifying Movie Reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM35K77CaawM",
        "outputId": "e2e9d3d9-2914-4d38-a27a-c9e58dd1e8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-24 09:45:37--  https://victorzhou.com/movie-reviews-dataset.zip\n",
            "Resolving victorzhou.com (victorzhou.com)... 104.21.72.186, 172.67.153.220, 2606:4700:3035::6815:48ba, ...\n",
            "Connecting to victorzhou.com (victorzhou.com)|104.21.72.186|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62951389 (60M) [application/zip]\n",
            "Saving to: ‘movie-reviews-dataset.zip.1’\n",
            "\n",
            "movie-reviews-datas 100%[===================>]  60.03M   243MB/s    in 0.2s    \n",
            "\n",
            "2024-12-24 09:45:38 (243 MB/s) - ‘movie-reviews-dataset.zip.1’ saved [62951389/62951389]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://victorzhou.com/movie-reviews-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ0YvOkpaeDQ",
        "outputId": "d20d2de8-7f5c-4da5-cfdc-21fd7c205883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  movie-reviews-dataset.zip\n",
            "replace movie-reviews-dataset/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip movie-reviews-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5aUOHnoWayK"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import keras as K\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from cv2 import resize\n",
        "from tensorflow.keras.preprocessing import text_dataset_from_directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3668d7a8"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "Performing a detailed analysis of the dataset to understand its structure and key features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5umg1bw2WtGs",
        "outputId": "a2c631fc-cda9-4a9c-f878-add9b690ab37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pos', 'neg', '.DS_Store']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "DATASET_DIR='/content/movie-reviews-dataset/train'\n",
        "os.listdir(DATASET_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BEQ_2Xkd_xe",
        "outputId": "6fa33431-3cf2-4f55-99cd-73e22532e1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Assumes you're in the root level of the dataset directory.\n",
        "# If you aren't, you'll need to change the relative paths here.\n",
        "train_data = text_dataset_from_directory('/content/movie-reviews-dataset/train')\n",
        "test_data = text_dataset_from_directory('/content/movie-reviews-dataset/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuLvOUq8ecxN",
        "outputId": "0d33973a-6e00-4c08-d56e-2236a1c971b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
        "from tensorflow.strings import regex_replace\n",
        "\n",
        "def prepareData(dir):\n",
        "  data = text_dataset_from_directory(dir)\n",
        "  return data.map(\n",
        "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
        "  )\n",
        "\n",
        "train_data = prepareData('/content/movie-reviews-dataset/train')\n",
        "test_data = prepareData('/content/movie-reviews-dataset/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d823c19"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Implementing NLP-specific preprocessing steps to prepare the data for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9JXNONUe2aD",
        "outputId": "fb2b0e24-0085-4de2-ccbc-872cfcc353d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"I thought the movie (especially the plot) needs a lot of work. The elements of the movie remains westernized and untrue to the attempt of trying to produce an eastern feel in the movie. I'll give three out of many of the flaws of the movie:  First, when Shen told Wendy that he would help her study the history of China, I was really happy that the audience would receive some information about Chinese history; but it turns out that the movie did not exactly show Wendy actually studying Chinese history; yet instead, the movie only shows Wendy practicing the method of remembering what she had studied, which frustrated and put me in dismay.  Second, which really bothered me, is how the characters kept mentioning about moon cakes -- moon cakes this and moon cakes that and how good it tastes. Yet they didn't really mention the real significance of it. The only they they talked about that had any relevance to the moon cake was the Autumn Festival, which they did not explain or go in depth. They could have mentioned the myth that correlates with the moon cake -- the Moon Lady. The myth starts of with how there once exists ten suns and each would rotate rising, but one day all ten suns rose up, drying up the land with the rising intense heat; so the Divine Archer, Hou Yi, shot nine of the ten suns, leaving only one sun (there are different versions where the Hou Yi shot the eight out of nine suns). Because of his heroic contribution, he was given the pill of immortality so he could live on forever in case the ten suns do rise up again, but his wife, Chang-O stole it. After stealing it, she fled to the moon, where she met a hare. She then came upon an idea and told the hare to pound the pill into many piece so she could spread the pill all over earth, giving everyone immortality. (There are a few variations of this story but throughout my childhood, I, most of the time, heard about this version). I thought details such as this would make the plot more culturally Chinese oriented.  The last thing I would point out is the last battle scene of the movie. The teachers that were possessed by the monks were fighting the Terra-cotta Warriors (the life-like statues of the soldiers) went against the idea of how important Chinese history is to the Chinese. The Terra-cotta Warrors serves as a connection of China's past and it was very westernized (where evil must be killed in anyway possible) that the monks in the movies were willing to destroy that connection. It would be understandable if Wendy, considering she is Chinese-American and doesn't have full Chinese knowledge, had no problem destroying these priceless artifacts.  The whole movie was westernized because it seemed that all the monks and Shen want to do is fight... I mean, it's rated TVPG due to violence, which goes against the Confucius thinking of cooperation and harmony. It would seem more accurate that the monks try to avoid violence and try to work things out peacefully before having to resort to violence.  All in all, all of or either of the producer, writer, or director did not do their research thoroughly and did a messy and effortless job instead. I would suggest that they either stop airing this movie or that they re-shoot the movie so it contains more accurate information; however, I would give it credit (2 stars) for removing one stereotype of Asians and Asian-Americans of being smart and quiet.\"\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_data.take(1):\n",
        "  print(text_batch.numpy()[0])\n",
        "  print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwvIN8tAe8uT"
      },
      "source": [
        "3. Building the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ce8399"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "Detailed explanation of the LSTM architecture used and its configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNLfzan7e92e"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,), dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPG9MfzYfGFK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "max_tokens = 1000\n",
        "max_len = 100\n",
        "vectorize_layer = TextVectorization(\n",
        "  # Max vocab size. Any words outside of the max_tokens most common ones\n",
        "  # will be treated the same way: as \"out of vocabulary\" (OOV) tokens.\n",
        "  max_tokens=max_tokens,\n",
        "  # Output integer indices, one per string token\n",
        "  output_mode=\"int\",\n",
        "  # Always pad or truncate to exactly this many tokens\n",
        "  output_sequence_length=max_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZRyvNHrfLnN"
      },
      "outputs": [],
      "source": [
        "# Call adapt(), which fits the TextVectorization layer to our text dataset.\n",
        "# This is when the max_tokens most common words (i.e. the vocabulary) are selected.\n",
        "train_texts = train_data.map(lambda text, label: text)\n",
        "vectorize_layer.adapt(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N07nZVVfP2U"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Previous layer: TextVectorization\n",
        "max_tokens = 1000\n",
        "# ...\n",
        "model.add(vectorize_layer)\n",
        "\n",
        "# Note that we're using max_tokens + 1 here, since there's an\n",
        "# out-of-vocabulary (OOV) token that gets added to the vocab.\n",
        "model.add(Embedding(max_tokens + 1, 128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4MgF-tKffgV"
      },
      "source": [
        " The Recurrent Laye"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmDZG276fZHa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# 64 is the \"units\" parameter, which is the\n",
        "# dimensionality of the output space.\n",
        "model.add(LSTM(64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jRu3RBZflH-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUMk9vMXfr_y"
      },
      "source": [
        "Compiling and training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e97666b8"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Training the LSTM model using a training dataset and monitoring performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Veq0D1TVfvkj"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='binary_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yypqHZ01f42T",
        "outputId": "0c39920d-e9a7-4d7b-9fd4-4b17c4d64a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 72ms/step - accuracy: 0.5768 - loss: 0.6717\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 69ms/step - accuracy: 0.6848 - loss: 0.5875\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 69ms/step - accuracy: 0.7927 - loss: 0.4408\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 68ms/step - accuracy: 0.8102 - loss: 0.4140\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 67ms/step - accuracy: 0.7903 - loss: 0.4403\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 68ms/step - accuracy: 0.8124 - loss: 0.4119\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 68ms/step - accuracy: 0.8278 - loss: 0.3811\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 66ms/step - accuracy: 0.8346 - loss: 0.3650\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 68ms/step - accuracy: 0.8440 - loss: 0.3477\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 66ms/step - accuracy: 0.8504 - loss: 0.3314\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d90943627a0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model.fit(train_data, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1430563"
      },
      "source": [
        "## Evaluation and Insights\n",
        "\n",
        "Evaluating the model's performance and discussing key findings and insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLFAi_nsgNwr",
        "outputId": "5963e882-3cb6-4ab9-c6bb-7328e7644f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518ms/step\n",
            "[[0.98522633]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "[[0.00559015]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(tf.data.Dataset.from_tensor_slices([\n",
        "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
        "]).batch(1))) # Batch the dataset\n",
        "print(model.predict(tf.data.Dataset.from_tensor_slices([\n",
        "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
        "]).batch(1))) # Batch the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6b3gYOPzIQr1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}